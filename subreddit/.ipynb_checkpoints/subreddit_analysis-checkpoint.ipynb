{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313b35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using custom module.\n",
    "import sys\n",
    "sys.path.append('../custom_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1aaf2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import connect_to_db as cn\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "import csv\n",
    "import parmap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5842b250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21370"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"select distinct community_id_fastgreedy_is community_id from nodes order by community_id;\"\n",
    "communities_df = cn.select_query_result_to_df(sql)\n",
    "communities = list(np.array(communities_df['community_id'].tolist()))\n",
    "len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bc1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi processing 시, 병목 현상을 최소화하기 위해 community_id를 shuffle.\n",
    "random.sample(communities, len(communities))\n",
    "communities = sorted(communities, key=lambda k: random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c941ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit_per_community_main(index):\n",
    "    if index % 2000 == 0:\n",
    "        start_index = index - 2000\n",
    "    else:\n",
    "        start_index = index - (index % 2000)\n",
    "    \n",
    "    result_for_csv = []\n",
    "    \n",
    "    for i in range(start_index, index):\n",
    "        sql = f\"select node_id from nodes where community_id_fastgreedy_is = {communities[i]};\"\n",
    "        nodes_df = cn.select_query_result_to_df(sql)\n",
    "        nodes = list(np.array(nodes_df['node_id'].tolist()))\n",
    "    \n",
    "        # query의 in 구문에 넣을 수 있도록 변형.\n",
    "        nodes = str(nodes)\n",
    "        nodes = nodes.lstrip('[')\n",
    "        nodes = nodes.rstrip(']')\n",
    "\n",
    "        # community의 user들이 서로 interaction한 comments, posts에 대해서만 subreddit count, entropy 구함.\n",
    "        sql2 = f\"select c.subreddit_key, count(*) from comments c inner join posts p on c.link_key = p.post_key where c.author in ({nodes}) and p.author in ({nodes}) and c.link_key = c.parent_key and c.is_valid=1 and c.author <> p.author group by c.subreddit_key;\"\n",
    "        subreddit_df = cn.select_query_result_to_df(sql2)\n",
    "    \n",
    "        # the number of subreddit by community\n",
    "        subreddit = list(np.array(subreddit_df['subreddit_key'].tolist()))\n",
    "        subreddits = len(subreddit)\n",
    "    \n",
    "        # entropy of subreddit by community\n",
    "        subreddit_count = list(np.array(subreddit_df['count(*)'].tolist()))\n",
    "        if subreddits == 1:\n",
    "            subreddit_entropy = 0\n",
    "        else:\n",
    "            subreddit_entropy = entropy(subreddit_count, base=subreddits)\n",
    "            \n",
    "        result_for_csv.append([communities[i], subreddits, subreddit_entropy])\n",
    "    \n",
    "    fields = ['community_id', 'subreddits', 'subreddit_entropy']\n",
    "    cn.write_csv_for_db_update(f\"../subreddit/csv/subreddit_analysis_{index}.csv\", fields, result_for_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa75638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# index_list = [2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 20000, 21370]\n",
    "index_list = [1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # multi processing\n",
    "    parmap.map(subreddit_per_community_main, index_list, pm_pbar=True, pm_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269944bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       community_id  subreddits  subreddit_entropy\n",
      "0              9062           2           0.970951\n",
      "1              9874           1           0.000000\n",
      "2             15712           1           0.000000\n",
      "3             10787           1           0.000000\n",
      "4             11783           1           0.000000\n",
      "...             ...         ...                ...\n",
      "21365         12120           1           0.000000\n",
      "21366         20177           1           0.000000\n",
      "21367          7281           1           0.000000\n",
      "21368          8222           1           0.000000\n",
      "21369          2515           1           0.000000\n",
      "\n",
      "[21370 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "subreddit_df = pd.read_csv('../subreddit/csv/subreddit_analysis.csv')\n",
    "print(subreddit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79531838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "0.004773046326626111\n"
     ]
    }
   ],
   "source": [
    "subreddits = list(np.array(subreddit_df['subreddits'].tolist()))\n",
    "count = 0\n",
    "for element in subreddits:\n",
    "    if element >= 5:\n",
    "        count += 1\n",
    "print(count)\n",
    "print(count / len(subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eadaa813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668\n",
      "0.07805334581188582\n"
     ]
    }
   ],
   "source": [
    "entropy = list(np.array(subreddit_df['subreddit_entropy'].tolist()))\n",
    "count = 0\n",
    "for element in entropy:\n",
    "    if element >= 0.8:\n",
    "        count += 1\n",
    "print(count)\n",
    "print(count / len(entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5ec5d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community_id          3.000000\n",
      "subreddits           47.000000\n",
      "subreddit_entropy     0.448251\n",
      "Name: 16156, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(subreddit_df)):\n",
    "    if subreddit_df.loc[i]['community_id'] == 3:\n",
    "        print(subreddit_df.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cba12740",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'select community_id_fastgreedy_is, count(*) from nodes group by community_id_fastgreedy_is having count(*) = 2;'\n",
    "community_2_df = cn.select_query_result_to_df(sql)\n",
    "community_2_list = list(np.array(community_2_df['community_id_fastgreedy_is'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e6a0193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14740.0 2.0\n",
      "18249.0 2.0\n",
      "13191.0 2.0\n",
      "410.0 2.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(subreddit_df)):\n",
    "    if subreddit_df.loc[i]['community_id'] in community_2_list:\n",
    "        if subreddit_df.loc[i]['subreddits'] != 1:\n",
    "            print(subreddit_df.loc[i]['community_id'], subreddit_df.loc[i]['subreddits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "722f3cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8229"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'select community_id_fastgreedy_is, count(*) from nodes group by community_id_fastgreedy_is having count(*) >= 3;'\n",
    "community_3_df = cn.select_query_result_to_df(sql)\n",
    "community_3_list = list(np.array(community_3_df['community_id_fastgreedy_is'].tolist()))\n",
    "len(community_3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c00590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for community in community_3_list:\n",
    "    df = subreddit_df.loc[subreddit_df['community_id'] == community]\n",
    "    value = df.iloc[0]['subreddit_entropy']\n",
    "    if value > 0.8:\n",
    "        result.append(value)\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c63f2b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546688"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_df.iloc[0]['subreddit_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9324037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5869\n"
     ]
    }
   ],
   "source": [
    "subreddit_one_count = 0\n",
    "for i in range(len(subreddit_df)):\n",
    "    if subreddit_df.loc[i]['community_id'] in community_3_list:\n",
    "        if subreddit_df.loc[i]['subreddits'] == 1:\n",
    "            subreddit_one_count += 1\n",
    "            \n",
    "print(subreddit_one_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c708a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"select community_id_fastgreedy_is community_id, count(*) from nodes group by community_id_fastgreedy_is order by count(*) desc limit 105;\"\n",
    "big_communities_df = cn.select_query_result_to_df(sql)\n",
    "big_communities = list(np.array(big_communities_df['community_id'].tolist()))\n",
    "len(big_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d5497ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for community in big_communities:\n",
    "    sql = f\"select node_id from nodes where community_id_fastgreedy_is = {community};\"\n",
    "    nodes_df = cn.select_query_result_to_df(sql)\n",
    "    nodes = list(np.array(nodes_df['node_id'].tolist()))\n",
    "    \n",
    "    # query의 in 구문에 넣을 수 있도록 변형.\n",
    "    nodes = str(nodes)\n",
    "    nodes = nodes.lstrip('[')\n",
    "    nodes = nodes.rstrip(']')\n",
    "\n",
    "    # community의 user들이 서로 interaction한 comments, posts에 대해서만 subreddit count, entropy 구함.\n",
    "    sql2 = f\"select p.post_key, c.comment_key from comments c inner join posts p on c.link_key = p.post_key where c.author in ({nodes}) and p.author in ({nodes}) and c.link_key = c.parent_key and c.is_valid=1 and c.author <> p.author;\"\n",
    "    result_df = cn.select_query_result_to_df(sql2)\n",
    "    result_list = list(np.array(result_df['post_key'].tolist()))\n",
    "    result.append(len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1e970604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[584427, 291892, 302251, 472303, 5000, 2173, 1899, 2015, 1599, 937, 300, 287, 206, 147, 139, 130, 130, 125, 141, 118, 111, 117, 105, 98, 99, 90, 88, 82, 81, 75, 72, 63, 62, 59, 59, 59, 53, 53, 51, 51, 48, 47, 50, 45, 46, 47, 45, 44, 44, 42, 42, 41, 40, 40, 39, 40, 45, 38, 40, 37, 39, 37, 35, 35, 37, 36, 34, 35, 35, 35, 35, 34, 35, 33, 33, 33, 33, 33, 36, 32, 34, 33, 31, 31, 34, 32, 30, 30, 32, 31, 30, 29, 30, 30, 30, 29, 28, 28, 30, 29, 28, 28, 28, 27, 27]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7cfb2499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = f\"select node_id from nodes where community_id_fastgreedy_is = {big_communities[100]};\"\n",
    "nodes_df = cn.select_query_result_to_df(sql)\n",
    "nodes = list(np.array(nodes_df['node_id'].tolist()))\n",
    "    \n",
    "# query의 in 구문에 넣을 수 있도록 변형.\n",
    "nodes = str(nodes)\n",
    "nodes = nodes.lstrip('[')\n",
    "nodes = nodes.rstrip(']')\n",
    "\n",
    "# community의 user들이 서로 interaction한 comments, posts에 대해서만 subreddit count, entropy 구함.\n",
    "sql2 = f\"select p.post_key, c.comment_key from comments c inner join posts p on c.link_key = p.post_key where c.author in ({nodes}) and p.author in ({nodes}) and c.link_key = c.parent_key and c.is_valid=1 and c.author <> p.author;\"\n",
    "result_df = cn.select_query_result_to_df(sql2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
